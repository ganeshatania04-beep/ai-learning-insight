{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yMF800eYle8",
        "outputId": "8da5b71b-810b-421c-c4ba-b116ea64bd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AI LEARNING INSIGHT - MODEL DEVELOPMENT\n",
            "================================================================================\n",
            "Libraries imported successfully\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from scipy import stats\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "import sys\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AI LEARNING INSIGHT - MODEL DEVELOPMENT\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Libraries imported successfully\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 1: LOADING DATA FROM DATA SPECIALIST\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Load scaled features (output dari data specialist)\n",
        "scaled_data = pd.read_csv('04_scaled_features_only_clean.csv')\n",
        "ml_ready = pd.read_csv('03_ml_ready_dataset_clean.csv')\n",
        "basic_summary = pd.read_csv('01_basic_user_summary_clean.csv', sep=';')\n",
        "\n",
        "# Load feature documentation\n",
        "with open('feature_documentation.json', 'r') as f:\n",
        "    feature_docs = json.load(f)\n",
        "\n",
        "# Load scalers\n",
        "standard_scaler = joblib.load('standard_scaler.pkl')\n",
        "minmax_scaler = joblib.load('minmax_scaler.pkl')\n",
        "\n",
        "print(f\"Scaled data shape: {scaled_data.shape}\")\n",
        "print(f\"ML ready shape: {ml_ready.shape}\")\n",
        "print(f\"Basic summary shape: {basic_summary.shape}\")\n",
        "print(\"Scalers loaded successfully\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvX5l6QvYvJz",
        "outputId": "8a3f4b26-387d-42c8-bf7d-353dce2e1790"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1: LOADING DATA FROM DATA SPECIALIST\n",
            "--------------------------------------------------------------------------------\n",
            "Scaled data shape: (8, 49)\n",
            "ML ready shape: (8, 77)\n",
            "Basic summary shape: (8, 11)\n",
            "Scalers loaded successfully\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 2: FEATURE PREPARATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Extract user IDs\n",
        "user_ids = scaled_data['user_id'].values\n",
        "\n",
        "# Get all numerical features (exclude user_id)\n",
        "feature_columns = [col for col in scaled_data.columns if col != 'user_id']\n",
        "X_clustering = scaled_data[feature_columns].values\n",
        "feature_names = feature_columns\n",
        "\n",
        "print(f\"Features selected: {len(feature_names)}\")\n",
        "print(f\"Feature matrix shape: {X_clustering.shape}\")\n",
        "print(f\"Sample features: {feature_names[:5]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-fK2seYy3T",
        "outputId": "a1df63df-0e58-46bb-d091-8a8ea53dd552"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 2: FEATURE PREPARATION\n",
            "--------------------------------------------------------------------------------\n",
            "Features selected: 48\n",
            "Feature matrix shape: (8, 48)\n",
            "Sample features: ['active_days_week_standard', 'consistency_score_standard', 'unique_tutorials_standard', 'total_accesses_standard', 'completed_count_standard']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 3: OPTIMAL K DETERMINATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "k_range = range(2, min(7, len(X_clustering)))\n",
        "metrics = {'k': [], 'wcss': [], 'silhouette': [], 'calinski': [], 'davies': []}\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    labels = kmeans.fit_predict(X_clustering)\n",
        "\n",
        "    metrics['k'].append(k)\n",
        "    metrics['wcss'].append(kmeans.inertia_)\n",
        "    metrics['silhouette'].append(silhouette_score(X_clustering, labels))\n",
        "    metrics['calinski'].append(calinski_harabasz_score(X_clustering, labels))\n",
        "    metrics['davies'].append(davies_bouldin_score(X_clustering, labels))\n",
        "\n",
        "    print(f\"k={k}: Silhouette={metrics['silhouette'][-1]:.3f}, \"\n",
        "          f\"WCSS={metrics['wcss'][-1]:.2f}\")\n",
        "\n",
        "# Select optimal k based on silhouette score\n",
        "optimal_idx = np.argmax(metrics['silhouette'])\n",
        "optimal_k = metrics['k'][optimal_idx]\n",
        "print(f\"\\nOptimal K selected: {optimal_k}\")\n",
        "print(f\"Best Silhouette Score: {metrics['silhouette'][optimal_idx]:.3f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdV6gJ_cY3BN",
        "outputId": "23d824e2-9c0c-4cd8-b0c0-f93a9493eec0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 3: OPTIMAL K DETERMINATION\n",
            "--------------------------------------------------------------------------------\n",
            "k=2: Silhouette=0.315, WCSS=96.45\n",
            "k=3: Silhouette=0.434, WCSS=50.44\n",
            "k=4: Silhouette=0.364, WCSS=34.26\n",
            "k=5: Silhouette=0.355, WCSS=19.83\n",
            "k=6: Silhouette=0.331, WCSS=9.66\n",
            "\n",
            "Optimal K selected: 3\n",
            "Best Silhouette Score: 0.434\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 4: TRAINING FINAL CLUSTERING MODEL\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Train final model\n",
        "final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, max_iter=300)\n",
        "cluster_labels = final_kmeans.fit_predict(X_clustering)\n",
        "\n",
        "# Calculate final metrics\n",
        "final_metrics = {\n",
        "    'n_clusters': optimal_k,\n",
        "    'silhouette_score': silhouette_score(X_clustering, cluster_labels),\n",
        "    'calinski_harabasz_score': calinski_harabasz_score(X_clustering, cluster_labels),\n",
        "    'davies_bouldin_score': davies_bouldin_score(X_clustering, cluster_labels),\n",
        "    'inertia': final_kmeans.inertia_,\n",
        "    'n_samples': len(X_clustering),\n",
        "    'n_features': X_clustering.shape[1],\n",
        "    'n_iter': final_kmeans.n_iter_\n",
        "}\n",
        "\n",
        "print(f\"Clustering completed:\")\n",
        "print(f\"  Silhouette Score: {final_metrics['silhouette_score']:.4f}\")\n",
        "print(f\"  Calinski-Harabasz: {final_metrics['calinski_harabasz_score']:.2f}\")\n",
        "print(f\"  Davies-Bouldin: {final_metrics['davies_bouldin_score']:.4f}\")\n",
        "print(f\"  Iterations: {final_metrics['n_iter']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1VrsH75Y7_s",
        "outputId": "4b73ed1e-7f2b-471c-c72a-ecfc60dc4173"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 4: TRAINING FINAL CLUSTERING MODEL\n",
            "--------------------------------------------------------------------------------\n",
            "Clustering completed:\n",
            "  Silhouette Score: 0.4342\n",
            "  Calinski-Harabasz: 5.47\n",
            "  Davies-Bouldin: 0.7587\n",
            "  Iterations: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 5: CONFIDENCE SCORE CALCULATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Calculate distances to cluster centers\n",
        "cluster_distances = final_kmeans.transform(X_clustering)\n",
        "min_distances = np.min(cluster_distances, axis=1)\n",
        "\n",
        "# Normalize to confidence score (0-1)\n",
        "max_distance = np.max(min_distances)\n",
        "confidence_scores = 1 - (min_distances / max_distance) if max_distance > 0 else np.ones(len(X_clustering))\n",
        "\n",
        "print(f\"Confidence scores calculated\")\n",
        "print(f\"  Range: {confidence_scores.min():.3f} - {confidence_scores.max():.3f}\")\n",
        "print(f\"  Mean: {confidence_scores.mean():.3f}\")\n",
        "\n",
        "# Confidence distribution\n",
        "high_conf = (confidence_scores >= 0.7).sum()\n",
        "med_conf = ((confidence_scores >= 0.5) & (confidence_scores < 0.7)).sum()\n",
        "low_conf = (confidence_scores < 0.5).sum()\n",
        "\n",
        "print(f\"  High (>=0.7): {high_conf} users ({high_conf/len(confidence_scores)*100:.1f}%)\")\n",
        "print(f\"  Medium (0.5-0.7): {med_conf} users ({med_conf/len(confidence_scores)*100:.1f}%)\")\n",
        "print(f\"  Low (<0.5): {low_conf} users ({low_conf/len(confidence_scores)*100:.1f}%)\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmVoUxXGZABE",
        "outputId": "238809e3-1642-4ec5-963c-79aa655b345a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 5: CONFIDENCE SCORE CALCULATION\n",
            "--------------------------------------------------------------------------------\n",
            "Confidence scores calculated\n",
            "  Range: 0.000 - 0.983\n",
            "  Mean: 0.381\n",
            "  High (>=0.7): 2 users (25.0%)\n",
            "  Medium (0.5-0.7): 0 users (0.0%)\n",
            "  Low (<0.5): 6 users (75.0%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 6: LEARNING PATTERN ASSIGNMENT\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "def assign_learning_patterns(scaled_data, cluster_labels, optimal_k):\n",
        "    \"\"\"Assign clean learning pattern results: only name + description.\"\"\"\n",
        "\n",
        "    patterns = {}\n",
        "    df_analysis = scaled_data.copy()\n",
        "    df_analysis['cluster'] = cluster_labels\n",
        "\n",
        "    feature_cols = [col for col in scaled_data.columns if col != 'cluster']\n",
        "\n",
        "    # Identify actual feature columns\n",
        "    consistency_col = [col for col in feature_cols if 'consistency' in col.lower()][0]\n",
        "    completion_col = [col for col in feature_cols if 'completion' in col.lower()][0]\n",
        "    intensity_col = [col for col in feature_cols if 'intensity' in col.lower()][0]\n",
        "\n",
        "    for cluster_id in range(optimal_k):\n",
        "        cluster_data = df_analysis[df_analysis['cluster'] == cluster_id]\n",
        "\n",
        "        consistency = cluster_data[consistency_col].mean()\n",
        "        completion = cluster_data[completion_col].mean()\n",
        "        intensity = cluster_data[intensity_col].mean()\n",
        "\n",
        "        # Pattern assignment logic\n",
        "        if consistency > 0.3 and completion >= 0:\n",
        "            pattern_name = \"Consistent Learner\"\n",
        "            description = (\n",
        "                \"Kamu tipe belajar yang slow but steady! Konsistensi kamu tuh keren banget. \"\n",
        "                \"Pelan tapi pasti. Setiap langkah kecil tetap punya impact, dan kamu selalu buktiin itu.\"\n",
        "            )\n",
        "        elif intensity > 0.2 and consistency > -0.5:\n",
        "            pattern_name = \"Fast Learner\"\n",
        "            description = (\n",
        "                \"Wah, kamu tipe speed learner! Materi baru langsung nyantol di otak. \"\n",
        "                \"Tapi tetep ya, jangan sampai kecepatan bikin kamu skip kualitas!\"\n",
        "            )\n",
        "        else:\n",
        "            pattern_name = \"Reflective Learner\"\n",
        "            description = (\n",
        "                \"Ini dia deep thinker! Kamu gak terburu-buru belajar, tapi setiap hal yang kamu pelajari \"\n",
        "                \"itu benar-benar kamu cerna sampai paham luar dalam.\"\n",
        "            )\n",
        "\n",
        "        patterns[cluster_id] = {\n",
        "            \"pattern_name\": pattern_name,\n",
        "            \"description\": description\n",
        "        }\n",
        "\n",
        "    return patterns\n",
        "\n",
        "\n",
        "learning_patterns = assign_learning_patterns(scaled_data, cluster_labels, optimal_k)\n",
        "\n",
        "print(\"Learning patterns identified:\")\n",
        "for cluster_id, pattern in learning_patterns.items():\n",
        "    print(f\"\\n  Cluster {cluster_id}: {pattern['pattern_name']}\")\n",
        "    print(f\"    Description: {pattern['description'][:80]}...\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fgPhB-4ZEDs",
        "outputId": "9cf0a5b1-4980-4188-ee3a-6b141787f6a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 6: LEARNING PATTERN ASSIGNMENT\n",
            "--------------------------------------------------------------------------------\n",
            "Learning patterns identified:\n",
            "\n",
            "  Cluster 0: Consistent Learner\n",
            "    Description: Kamu tipe belajar yang slow but steady! Konsistensi kamu tuh keren banget. Pelan...\n",
            "\n",
            "  Cluster 1: Fast Learner\n",
            "    Description: Wah, kamu tipe speed learner! Materi baru langsung nyantol di otak. Tapi tetep y...\n",
            "\n",
            "  Cluster 2: Fast Learner\n",
            "    Description: Wah, kamu tipe speed learner! Materi baru langsung nyantol di otak. Tapi tetep y...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_consistency_description(score):\n",
        "    if score > 0.3:\n",
        "        return {\n",
        "            'category': \"High Consistency\",\n",
        "            'description': \"Konsistensi kamu kelas expert! Disiplin banget sampai belajar udah kayak bagian dari rutinitas harian kamu.\"\n",
        "        }\n",
        "    elif score > -0.5:\n",
        "        return {\n",
        "            'category': \"Medium Consistency\",\n",
        "            'description': \"Cukup konsisten! Kadang ada jeda kecil, tapi overall kamu masih on track. Bisa makin stabil kalau dibantu reminder atau mini-habit.\"\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'category': \"Low Consistency\",\n",
        "            'description': \"Belajar kamu masih agak acak nih. Wajar kalau lagi banyak distraksi. Yuk pelan-pelan bangun habit biar ritme makin stabil.\"\n",
        "        }"
      ],
      "metadata": {
        "id": "9sZCvPmGahdk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_time_description(time_period):\n",
        "    \"\"\"Map time period to description\"\"\"\n",
        "    descriptions = {\n",
        "        'Morning': {\n",
        "            'period_name': 'The Early Bird',\n",
        "            'time_range': '5 AM - 12 PM',\n",
        "            'description': 'Kamu di pagi hari tuh kayak HP habis di-charge semalaman. Full battery dan fokus maksimal. Dunia masih sepi, vibe masih calm, dan itu bikin kamu gampang banget nyerap materi.'\n",
        "        },\n",
        "        'Afternoon': {\n",
        "            'period_name': 'The Prime-Time Learner',\n",
        "            'time_range': '12 PM - 5 PM',\n",
        "            'description': 'Siang hari itu prime time kamu! Kepala udah gak ngantuk pagi, tapi juga belum masuk mode capek. Kombinasi perfect buat belajar dengan santai tapi tetap produktif.'\n",
        "        },\n",
        "        'Evening': {\n",
        "            'period_name': 'The Sunset Scholar',\n",
        "            'time_range': '5 PM - 10 PM',\n",
        "            'description': 'Kamu tipe yang perlu warm-up dulu sebelum masuk learning mode. Pas sore ke malam, kamu baru dapet vibe yang pas lebih tenang, lebih fokus, dan akhirnya produktif banget. Evening study hits different buat kamu!'\n",
        "        },\n",
        "        'Late Night': {\n",
        "            'period_name': 'The Night Owl',\n",
        "            'time_range': '10 PM - 5 AM',\n",
        "            'description': 'Kamu belajar saat orang lain udah tepar. Malem sunyi banget, dan itu bikin fokus kamu meningkat tajam. Cuma jangan lupa, quality sleep tetap penting ya! Biar gak jadi zombie keesokan harinya~'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return descriptions.get(time_period, {\n",
        "        'period_name': 'Unknown',\n",
        "        'time_range': 'Unknown',\n",
        "        'description': 'Data time period not available.'\n",
        "    })"
      ],
      "metadata": {
        "id": "WYMAoYO0bHfa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 7: PREDICTIVE MODEL TRAINING\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Prepare data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_clustering, cluster_labels, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    'random_forest': RandomForestClassifier(n_estimators=50, random_state=42),\n",
        "    'knn': KNeighborsClassifier(n_neighbors=3),\n",
        "    'logistic': LogisticRegression(random_state=42, max_iter=1000)\n",
        "}\n",
        "\n",
        "best_score = 0\n",
        "best_model_name = None\n",
        "best_model = None\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if accuracy > best_score:\n",
        "        best_score = accuracy\n",
        "        best_model_name = name\n",
        "        best_model = model\n",
        "\n",
        "    print(f\"{name}: Accuracy={accuracy:.3f}\")\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} (Accuracy: {best_score:.3f})\")\n",
        "\n",
        "# Train final predictor on all data\n",
        "final_predictor = models[best_model_name]\n",
        "final_predictor.fit(X_clustering, cluster_labels)\n",
        "print(\"Final predictor trained on full dataset\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egG0cSzrbK3_",
        "outputId": "a8f5d6df-3a94-4acb-cc00-8cca82010f1c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 7: PREDICTIVE MODEL TRAINING\n",
            "--------------------------------------------------------------------------------\n",
            "random_forest: Accuracy=0.333\n",
            "knn: Accuracy=0.000\n",
            "logistic: Accuracy=0.333\n",
            "\n",
            "Best model: random_forest (Accuracy: 0.333)\n",
            "Final predictor trained on full dataset\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 8: USER INSIGHTS GENERATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get consistency column only (since only 3 features needed)\n",
        "consistency_col = [col for col in scaled_data.columns if 'consistency' in col.lower()][0]\n",
        "\n",
        "user_insights = []\n",
        "\n",
        "for idx in range(len(scaled_data)):\n",
        "    user_id = user_ids[idx]\n",
        "    cluster_id = cluster_labels[idx]\n",
        "    pattern = learning_patterns[cluster_id]\n",
        "\n",
        "    # Get user info\n",
        "    user_info = basic_summary[basic_summary['User ID'] == user_id]\n",
        "    display_name = user_info['Display Name'].iloc[0] if len(user_info) else 'Unknown'\n",
        "    most_active_time = user_info['Most Active Time'].iloc[0] if len(user_info) else 'Unknown'\n",
        "\n",
        "    # Consistency score\n",
        "    consistency_score = float(scaled_data.iloc[idx][consistency_col])\n",
        "\n",
        "    # Descriptions\n",
        "    time_description = get_active_time_description(most_active_time)\n",
        "    consistency_description = get_consistency_description(consistency_score)\n",
        "\n",
        "    # Final output with ONLY 3 FEATURES\n",
        "    insight = {\n",
        "        'user_id': int(user_id),\n",
        "        'display_name': display_name,\n",
        "\n",
        "        # 1. Learning Pattern\n",
        "        'learning_pattern': {\n",
        "            'name': pattern['pattern_name'],\n",
        "            'description': pattern['description'],\n",
        "        },\n",
        "\n",
        "        # 2. Most Active Time\n",
        "        'most_active_time': {\n",
        "            'period': most_active_time,\n",
        "            'period_name': time_description['period_name'],\n",
        "            'description': time_description['description']\n",
        "        },\n",
        "\n",
        "        # 3. Consistency Score\n",
        "        'consistency': {\n",
        "    'category': consistency_description['category'],\n",
        "    'description': consistency_description['description']\n",
        "}\n",
        "\n",
        "\n",
        "    }\n",
        "\n",
        "    user_insights.append(insight)\n",
        "\n",
        "print(f\"Generated insights for {len(user_insights)} users\")\n",
        "print(\"\\nUser insight:\")\n",
        "print(json.dumps(user_insights[0], indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTHoAnTIbZKF",
        "outputId": "1b819cce-8583-4606-9623-14b084b23492"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 8: USER INSIGHTS GENERATION\n",
            "--------------------------------------------------------------------------------\n",
            "Generated insights for 8 users\n",
            "\n",
            "User insight:\n",
            "{\n",
            "    \"user_id\": 96989,\n",
            "    \"display_name\": \"igihcksn\",\n",
            "    \"learning_pattern\": {\n",
            "        \"name\": \"Fast Learner\",\n",
            "        \"description\": \"Wah, kamu tipe speed learner! Materi baru langsung nyantol di otak. Tapi tetep ya, jangan sampai kecepatan bikin kamu skip kualitas!\"\n",
            "    },\n",
            "    \"most_active_time\": {\n",
            "        \"period\": \"Afternoon\",\n",
            "        \"period_name\": \"The Prime-Time Learner\",\n",
            "        \"description\": \"Siang hari itu prime time kamu! Kepala udah gak ngantuk pagi, tapi juga belum masuk mode capek. Kombinasi perfect buat belajar dengan santai tapi tetap produktif.\"\n",
            "    },\n",
            "    \"consistency\": {\n",
            "        \"category\": \"High Consistency\",\n",
            "        \"description\": \"Konsistensi kamu kelas expert! Disiplin banget sampai belajar udah kayak bagian dari rutinitas harian kamu.\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 9: TIME PERIOD DISTRIBUTION CALCULATION\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Load tracking data dari Excel\n",
        "trackings_time = pd.read_excel('developer_journey_trackings.xlsx')\n",
        "print(f\"Loaded {len(trackings_time)} tracking records\")\n",
        "\n",
        "# Process datetime\n",
        "trackings_time['last_viewed'] = pd.to_datetime(trackings_time['last_viewed'])\n",
        "trackings_time['hour'] = trackings_time['last_viewed'].dt.hour\n",
        "\n",
        "def get_time_period(hour):\n",
        "    if pd.isna(hour): return None\n",
        "    if 5 <= hour < 12: return 'Morning'\n",
        "    elif 12 <= hour < 17: return 'Afternoon'\n",
        "    elif 17 <= hour < 22: return 'Evening'\n",
        "    else: return 'Late Night'\n",
        "\n",
        "trackings_time['time_period'] = trackings_time['hour'].apply(get_time_period)\n",
        "\n",
        "# Ensure correct column name\n",
        "if 'developer_id' in trackings_time.columns:\n",
        "    trackings_time.rename(columns={'developer_id': 'user_id'}, inplace=True)\n",
        "\n",
        "# Calculate distribution per user\n",
        "time_distribution = (\n",
        "    trackings_time.groupby(['user_id', 'time_period'])\n",
        "    .size()\n",
        "    .reset_index(name='access_count')\n",
        ")\n",
        "\n",
        "# Pivot to wide format\n",
        "time_dist_pivot = time_distribution.pivot(\n",
        "    index='user_id',\n",
        "    columns='time_period',\n",
        "    values='access_count'\n",
        ").fillna(0).astype(int).reset_index()\n",
        "\n",
        "# Ensure all periods exist\n",
        "for period in ['Morning', 'Afternoon', 'Evening', 'Late Night']:\n",
        "    if period not in time_dist_pivot.columns:\n",
        "        time_dist_pivot[period] = 0\n",
        "\n",
        "# Calculate percentages\n",
        "time_dist_pivot['total'] = (\n",
        "    time_dist_pivot['Morning'] + time_dist_pivot['Afternoon'] +\n",
        "    time_dist_pivot['Evening'] + time_dist_pivot['Late Night']\n",
        ")\n",
        "\n",
        "for period in ['Morning', 'Afternoon', 'Evening', 'Late Night']:\n",
        "    time_dist_pivot[f'{period}_percentage'] = (\n",
        "        (time_dist_pivot[period] / time_dist_pivot['total'] * 100)\n",
        "        .round(1)\n",
        "        .fillna(0)\n",
        "    )\n",
        "\n",
        "print(f\"Time distribution calculated for {len(time_dist_pivot)} users\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLeXKNRZc_0V",
        "outputId": "418fc89f-3ff1-463b-8288-91d6c05914e8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 9: TIME PERIOD DISTRIBUTION CALCULATION\n",
            "--------------------------------------------------------------------------------\n",
            "Loaded 6641 tracking records\n",
            "Time distribution calculated for 8 users\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP 10: SAVE MODELS & ARTIFACTS\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'ml_models_output'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# 1. Save clustering model\n",
        "joblib.dump(final_kmeans, f'{output_dir}/clustering_model.pkl')\n",
        "print(\"Clustering model saved\")\n",
        "\n",
        "# 2. Save predictor model\n",
        "joblib.dump(final_predictor, f'{output_dir}/predictor_model.pkl')\n",
        "print(\"Predictor model saved\")\n",
        "\n",
        "# 3. Save learning patterns\n",
        "with open(f'{output_dir}/learning_patterns.json', 'w') as f:\n",
        "    json.dump(learning_patterns, f, indent=4)\n",
        "print(\"Learning patterns saved\")\n",
        "\n",
        "# 4. Save user insights (with only 3 features)\n",
        "with open(f'{output_dir}/user_insights.json', 'w') as f:\n",
        "    json.dump(user_insights, f, indent=4)\n",
        "print(\"User insights saved\")\n",
        "\n",
        "# 5. Save clustering results\n",
        "results_df = pd.DataFrame({\n",
        "    'user_id': user_ids,\n",
        "    'cluster': cluster_labels,\n",
        "    'confidence_score': confidence_scores,\n",
        "    'learning_pattern': [learning_patterns[c]['pattern_name'] for c in cluster_labels]\n",
        "})\n",
        "results_df.to_csv(f'{output_dir}/clustering_results.csv', index=False)\n",
        "print(\"Clustering results saved\")\n",
        "\n",
        "# 6. Save time distribution\n",
        "time_dist_pivot.to_csv(f'{output_dir}/time_distribution.csv', index=False)\n",
        "print(\"Time distribution saved\")\n",
        "\n",
        "# 7. Save model metadata\n",
        "metadata = {\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'n_samples': len(X_clustering),\n",
        "    'n_features': X_clustering.shape[1],\n",
        "    'optimal_k': optimal_k,\n",
        "    'feature_names': feature_names,\n",
        "    'metrics': final_metrics,\n",
        "    'confidence_distribution': {\n",
        "        'high_confidence': int(high_conf),\n",
        "        'medium_confidence': int(med_conf),\n",
        "        'low_confidence': int(low_conf)\n",
        "    },\n",
        "    'predictor_model': best_model_name,\n",
        "    'predictor_accuracy': float(best_score)\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}/model_metadata.json', 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "print(\"Model metadata saved\")\n",
        "\n",
        "# 8. Save API documentation\n",
        "api_doc = {\n",
        "    'version': '1.0',\n",
        "    'endpoints': {\n",
        "        'user_insight': {\n",
        "            'method': 'GET',\n",
        "            'path': '/api/v1/user-insight/{user_id}',\n",
        "            'description': 'Get complete user insight with 3 features: Learning Pattern, Most Active Time, and Consistency',\n",
        "            'output_features': {\n",
        "                'learning_pattern': {\n",
        "                    'name': 'string',\n",
        "                    'description': 'string'\n",
        "                },\n",
        "                'most_active_time': {\n",
        "                    'period': 'string',\n",
        "                    'period_name': 'string',\n",
        "                    'description': 'string'\n",
        "                },\n",
        "                'consistency': {\n",
        "                    'category': 'string',\n",
        "                    'description': 'string'\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'time_distribution': {\n",
        "            'method': 'GET',\n",
        "            'path': '/api/v1/time-distribution/{user_id}',\n",
        "            'description': 'Get time period distribution for a user',\n",
        "            'output': {\n",
        "                'user_id': 'integer',\n",
        "                'Morning': 'integer (count)',\n",
        "                'Afternoon': 'integer (count)',\n",
        "                'Evening': 'integer (count)',\n",
        "                'Late Night': 'integer (count)',\n",
        "                'Morning_percentage': 'float',\n",
        "                'Afternoon_percentage': 'float',\n",
        "                'Evening_percentage': 'float',\n",
        "                'Late Night_percentage': 'float',\n",
        "                'total': 'integer'\n",
        "            }\n",
        "        },\n",
        "        'predict_cluster': {\n",
        "            'method': 'POST',\n",
        "            'path': '/api/v1/predict-cluster',\n",
        "            'description': 'Predict cluster for new user based on features',\n",
        "            'input': 'Array of 48 scaled features',\n",
        "            'output': {\n",
        "                'predicted_cluster': 'integer',\n",
        "                'learning_pattern_name': 'string',\n",
        "                'confidence_score': 'float (0-1)'\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    'models': {\n",
        "        'clustering': 'KMeans',\n",
        "        'predictor': best_model_name,\n",
        "        'n_clusters': optimal_k\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{output_dir}/api_documentation.json', 'w') as f:\n",
        "    json.dump(api_doc, f, indent=4)\n",
        "print(\"API documentation saved\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ALL MODELS & ARTIFACTS SAVED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Output directory: {output_dir}/\")\n",
        "print(f\"Files saved: 8\")\n",
        "print(\"\\nFiles:\")\n",
        "print(\"  1. clustering_model.pkl\")\n",
        "print(\"  2. predictor_model.pkl\")\n",
        "print(\"  3. learning_patterns.json\")\n",
        "print(\"  4. user_insights.json\")\n",
        "print(\"  5. clustering_results.csv\")\n",
        "print(\"  6. time_distribution.csv\")\n",
        "print(\"  7. model_metadata.json\")\n",
        "print(\"  8. api_documentation.json\")\n",
        "print(\"\\nReady for deployment!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daAF8Ag0dLfd",
        "outputId": "06e03bdf-e2df-4ac9-f7e8-66bfd40bba6c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 10: SAVE MODELS & ARTIFACTS\n",
            "--------------------------------------------------------------------------------\n",
            "Clustering model saved\n",
            "Predictor model saved\n",
            "Learning patterns saved\n",
            "User insights saved\n",
            "Clustering results saved\n",
            "Time distribution saved\n",
            "Model metadata saved\n",
            "API documentation saved\n",
            "\n",
            "================================================================================\n",
            "ALL MODELS & ARTIFACTS SAVED SUCCESSFULLY\n",
            "================================================================================\n",
            "Output directory: ml_models_output/\n",
            "Files saved: 8\n",
            "\n",
            "Files:\n",
            "  1. clustering_model.pkl\n",
            "  2. predictor_model.pkl\n",
            "  3. learning_patterns.json\n",
            "  4. user_insights.json\n",
            "  5. clustering_results.csv\n",
            "  6. time_distribution.csv\n",
            "  7. model_metadata.json\n",
            "  8. api_documentation.json\n",
            "\n",
            "Ready for deployment!\n"
          ]
        }
      ]
    }
  ]
}